{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import gym\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import numba as nb\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import error, spaces\n",
    "\n",
    "class Env(object):\n",
    "    def __init__(self):\n",
    "        self.reward_range = (-np.inf, np.inf)\n",
    "        self.action_space = spaces.Box(low=0., high=1., shape=(1,))\n",
    "        self.observation_space = spaces.Box(low=0., high=1., shape=(1,))\n",
    "\n",
    "    def step(self, action):\n",
    "        self.step_num += 1\n",
    "        done = False\n",
    "        self.actions.append(action)\n",
    "        \n",
    "        previous_obs = self.observation.copy()\n",
    "        \n",
    "        reward = 0.001/(abs(self.observation[0] - action[0]) + 0.001)\n",
    "        \n",
    "        #if abs(self.observation[0] - action[0]) < 0.001:\n",
    "        #    reward = 1.\n",
    "        #else:\n",
    "        #    reward = 0.\n",
    "            \n",
    "        #self.observation[0] = np.random.rand()\n",
    "\n",
    "        if self.step_num > 100:\n",
    "            done = True\n",
    "        \n",
    "        return self.observation, reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.step_num = 0\n",
    "        self.observation = np.array([0.2])\n",
    "        #self.observation = np.random.rand(1)\n",
    "        self.actions = []\n",
    "        return self.observation\n",
    "\n",
    "    def render(self, mode='human', close=False):\n",
    "        print(np.mean(self.actions))\n",
    "    \n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        pass\n",
    "\n",
    "    def configure(self, *args, **kwargs):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()\n",
    "\n",
    "    def __str__(self):\n",
    "        return '<{} instance>'.format(type(self).__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = 1000000\n",
    "\n",
    "LOSS_CLIPPING = 0.2 # Only implemented clipping for the surrogate loss, paper said it was best\n",
    "EPOCHS = 100\n",
    "NOISE = 0.01\n",
    "\n",
    "GAMMA = 0.99\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "NUM_ACTIONS = 1\n",
    "NUM_STATE = 1\n",
    "HIDDEN_SIZE = 256\n",
    "ENTROPY_LOSS = 5 * 1e-3 # Does not converge without entropy penalty\n",
    "LR = 1e-4 # Lower lr stabilises training greatly\n",
    "\n",
    "DUMMY_ACTION, DUMMY_VALUE = np.zeros((1, NUM_ACTIONS)), np.zeros((1, 1))\n",
    "\n",
    "def proximal_policy_optimization_loss_continuous(advantage, old_prediction):\n",
    "    def loss(y_true, y_pred):\n",
    "        var = K.square(NOISE)\n",
    "        pi = 3.1415926\n",
    "        denom = K.sqrt(2 * pi * var)\n",
    "        prob_num = K.exp(- K.square(y_true - y_pred)/ (2 * var))\n",
    "        old_prob_num = K.exp(- K.square(y_true - old_prediction)/ (2 * var))\n",
    "\n",
    "        prob = prob_num/denom\n",
    "        old_prob = old_prob_num/denom\n",
    "        r = prob/(old_prob + 1e-10)\n",
    "\n",
    "        return -K.mean(K.minimum(r * advantage, K.clip(r, min_value=1 - LOSS_CLIPPING, max_value=1 + LOSS_CLIPPING) * advantage))\n",
    "    return loss\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.critic = self.build_critic()\n",
    "        self.actor = self.build_actor_continuous()\n",
    "\n",
    "        self.env = Env()\n",
    "        print(self.env.action_space, 'action_space', self.env.observation_space, 'observation_space')\n",
    "        self.episode = 0\n",
    "        self.observation = self.env.reset()\n",
    "        self.reward = []\n",
    "        self.reward_over_time = []\n",
    "        self.writer = SummaryWriter('AllRuns/continuous/' + str(int(time.time())))\n",
    "        self.gradient_steps = 0\n",
    "\n",
    "    def build_actor_continuous(self):\n",
    "        state_input = Input(shape=(NUM_STATE,))\n",
    "        advantage = Input(shape=(1,))\n",
    "        old_prediction = Input(shape=(NUM_ACTIONS,))\n",
    "\n",
    "        x = Dense(HIDDEN_SIZE, activation='relu')(state_input)\n",
    "        x = Dropout(0.5)(x)\n",
    "        x = Dense(HIDDEN_SIZE, activation='relu')(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "\n",
    "        out_actions = Dense(NUM_ACTIONS, name='output')(x)\n",
    "\n",
    "        model = Model(inputs=[state_input, advantage, old_prediction], outputs=[out_actions])\n",
    "        model.compile(optimizer=Adam(lr=LR),\n",
    "                      loss=[proximal_policy_optimization_loss_continuous(\n",
    "                          advantage=advantage,\n",
    "                          old_prediction=old_prediction)])\n",
    "        model.summary()\n",
    "\n",
    "        return model\n",
    "\n",
    "    def build_critic(self):\n",
    "\n",
    "        state_input = Input(shape=(NUM_STATE,))\n",
    "        x = Dense(HIDDEN_SIZE, activation='relu')(state_input)\n",
    "        x = Dropout(0.5)(x)\n",
    "        x = Dense(HIDDEN_SIZE, activation='relu')(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "\n",
    "        out_value = Dense(1)(x)\n",
    "\n",
    "        model = Model(inputs=[state_input], outputs=[out_value])\n",
    "        model.compile(optimizer=Adam(lr=LR), loss='mse')\n",
    "\n",
    "        return model\n",
    "\n",
    "    def reset_env(self):\n",
    "        self.episode += 1\n",
    "        self.observation = self.env.reset()\n",
    "        self.reward = []\n",
    "\n",
    "    def get_action_continuous(self):\n",
    "        p = self.actor.predict([self.observation.reshape(1, NUM_STATE), DUMMY_VALUE, DUMMY_ACTION])\n",
    "        action = action_matrix = p[0] + np.random.normal(loc=0, scale=NOISE, size=p[0].shape)\n",
    "        return action, action_matrix, p\n",
    "\n",
    "    def transform_reward(self):\n",
    "        if self.episode % 100 == 0:\n",
    "            print('Episode #', self.episode, '\\tfinished with reward', np.array(self.reward).sum(),\n",
    "                  '\\tAverage reward of last 100 episode :', np.mean(self.reward_over_time[-100:]))\n",
    "        self.reward_over_time.append(np.array(self.reward).sum())\n",
    "        self.writer.add_scalar('Episode reward', np.array(self.reward).sum(), self.episode)\n",
    "        for j in range(len(self.reward) - 2, -1, -1):\n",
    "            self.reward[j] += self.reward[j + 1] * GAMMA\n",
    "\n",
    "    def get_batch(self):\n",
    "        batch = [[], [], [], []]\n",
    "\n",
    "        tmp_batch = [[], [], []]\n",
    "        while len(batch[0]) < BATCH_SIZE:\n",
    "            action, action_matrix, predicted_action = self.get_action_continuous()\n",
    "            observation, reward, done, info = self.env.step(action)\n",
    "            self.reward.append(reward)\n",
    "\n",
    "            tmp_batch[0].append(self.observation)\n",
    "            tmp_batch[1].append(action_matrix)\n",
    "            tmp_batch[2].append(predicted_action)\n",
    "            self.observation = observation\n",
    "\n",
    "            if done:\n",
    "                self.transform_reward()\n",
    "                for i in range(len(tmp_batch[0])):\n",
    "                    obs, action, pred = tmp_batch[0][i], tmp_batch[1][i], tmp_batch[2][i]\n",
    "                    r = self.reward[i]\n",
    "                    batch[0].append(obs)\n",
    "                    batch[1].append(action)\n",
    "                    batch[2].append(pred)\n",
    "                    batch[3].append(r)\n",
    "                tmp_batch = [[], [], []]\n",
    "                self.reset_env()\n",
    "\n",
    "        obs, action, pred, reward = np.array(batch[0]), np.array(batch[1]), np.array(batch[2]), np.reshape(np.array(batch[3]), (len(batch[3]), 1))\n",
    "        pred = np.reshape(pred, (pred.shape[0], pred.shape[2]))\n",
    "        return obs, action, pred, reward\n",
    "\n",
    "    def run(self):\n",
    "        while self.episode < EPISODES:\n",
    "            obs, action, pred, reward = self.get_batch()\n",
    "            old_prediction = pred\n",
    "            pred_values = self.critic.predict(obs)\n",
    "\n",
    "            advantage = reward - pred_values\n",
    "\n",
    "            actor_loss = []\n",
    "            critic_loss = []\n",
    "            for e in range(EPOCHS):\n",
    "                actor_loss.append(self.actor.train_on_batch([obs, advantage, old_prediction], [action]))\n",
    "                critic_loss.append(self.critic.train_on_batch([obs], [reward]))\n",
    "            self.writer.add_scalar('Actor loss', np.mean(actor_loss), self.gradient_steps)\n",
    "            self.writer.add_scalar('Critic loss', np.mean(critic_loss), self.gradient_steps)\n",
    "            self.writer.add_scalars('Action Observation', \n",
    "                                    {'action': np.mean(action),\n",
    "                                     'observation': np.mean(obs)}, self.gradient_steps)\n",
    "\n",
    "            self.gradient_steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_66 (InputLayer)        (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_84 (Dense)             (None, 256)               512       \n",
      "_________________________________________________________________\n",
      "dropout_67 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_85 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_68 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 66,561\n",
      "Trainable params: 66,561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Box(1,) action_space Box(1,) observation_space\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nathan/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/nathan/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode # 0 \tfinished with reward 0.48166654779542317 \tAverage reward of last 100 episode : nan\n",
      "Episode # 100 \tfinished with reward 0.5022418277375246 \tAverage reward of last 100 episode : 0.49906782058428223\n",
      "Episode # 200 \tfinished with reward 0.504733924109995 \tAverage reward of last 100 episode : 0.5080676660050569\n",
      "Episode # 300 \tfinished with reward 0.5251881125053428 \tAverage reward of last 100 episode : 0.5128887165166031\n",
      "Episode # 400 \tfinished with reward 0.5335635742130193 \tAverage reward of last 100 episode : 0.528298173602544\n",
      "Episode # 500 \tfinished with reward 0.5287803967248712 \tAverage reward of last 100 episode : 0.5280077922009377\n",
      "Episode # 600 \tfinished with reward 0.5136729661410431 \tAverage reward of last 100 episode : 0.5245889480784267\n",
      "Episode # 700 \tfinished with reward 0.517708411517717 \tAverage reward of last 100 episode : 0.5170960958936796\n",
      "Episode # 800 \tfinished with reward 0.5442012074629949 \tAverage reward of last 100 episode : 0.5357967412119198\n",
      "Episode # 900 \tfinished with reward 0.5311989503738399 \tAverage reward of last 100 episode : 0.5323251978159111\n",
      "Episode # 1000 \tfinished with reward 0.5257453024705151 \tAverage reward of last 100 episode : 0.5222412060824972\n",
      "Episode # 1100 \tfinished with reward 0.5020709956859475 \tAverage reward of last 100 episode : 0.5157412049411851\n",
      "Episode # 1200 \tfinished with reward 0.5110697251692247 \tAverage reward of last 100 episode : 0.503784744432325\n",
      "Episode # 1300 \tfinished with reward 0.49119284006181624 \tAverage reward of last 100 episode : 0.5065889646516566\n",
      "Episode # 1400 \tfinished with reward 0.5034576918226603 \tAverage reward of last 100 episode : 0.49307610341754654\n",
      "Episode # 1500 \tfinished with reward 0.5192539743061813 \tAverage reward of last 100 episode : 0.520102882655653\n",
      "Episode # 1600 \tfinished with reward 0.5225164408196932 \tAverage reward of last 100 episode : 0.5161836354048708\n",
      "Episode # 1700 \tfinished with reward 0.5201260115110069 \tAverage reward of last 100 episode : 0.5134547095584252\n",
      "Episode # 1800 \tfinished with reward 0.5332510594861045 \tAverage reward of last 100 episode : 0.5192033549821228\n",
      "Episode # 1900 \tfinished with reward 0.5362481079227448 \tAverage reward of last 100 episode : 0.5354845167441912\n",
      "Episode # 2000 \tfinished with reward 0.547332631640501 \tAverage reward of last 100 episode : 0.5445237439195388\n",
      "Episode # 2100 \tfinished with reward 0.5505339955320538 \tAverage reward of last 100 episode : 0.5554689225366052\n",
      "Episode # 2200 \tfinished with reward 0.5543489077878899 \tAverage reward of last 100 episode : 0.5565726553562613\n",
      "Episode # 2300 \tfinished with reward 0.5517328624836155 \tAverage reward of last 100 episode : 0.5537249158401572\n",
      "Episode # 2400 \tfinished with reward 0.5609969103650415 \tAverage reward of last 100 episode : 0.5589565128026348\n",
      "Episode # 2500 \tfinished with reward 0.5773413369436995 \tAverage reward of last 100 episode : 0.5660072592203722\n",
      "Episode # 2600 \tfinished with reward 0.602475677496682 \tAverage reward of last 100 episode : 0.5887585220661953\n",
      "Episode # 2700 \tfinished with reward 0.6060918559492456 \tAverage reward of last 100 episode : 0.6157546240156562\n",
      "Episode # 2800 \tfinished with reward 0.5877153420452627 \tAverage reward of last 100 episode : 0.6000735163133978\n",
      "Episode # 2900 \tfinished with reward 0.56154946402421 \tAverage reward of last 100 episode : 0.5730438318125087\n",
      "Episode # 3000 \tfinished with reward 0.5426556760541595 \tAverage reward of last 100 episode : 0.5425111989377791\n",
      "Episode # 3100 \tfinished with reward 0.5452699873949417 \tAverage reward of last 100 episode : 0.5472804054353858\n",
      "Episode # 3200 \tfinished with reward 0.5527544924393962 \tAverage reward of last 100 episode : 0.5434597691238577\n",
      "Episode # 3300 \tfinished with reward 0.5486824794323512 \tAverage reward of last 100 episode : 0.5529403855364631\n",
      "Episode # 3400 \tfinished with reward 0.5461148105629652 \tAverage reward of last 100 episode : 0.5486491176674528\n",
      "Episode # 3500 \tfinished with reward 0.5863472160412675 \tAverage reward of last 100 episode : 0.5619803061869478\n",
      "Episode # 3600 \tfinished with reward 0.578610466108985 \tAverage reward of last 100 episode : 0.5742948486303159\n",
      "Episode # 3700 \tfinished with reward 0.5822903062713867 \tAverage reward of last 100 episode : 0.5733517567141309\n",
      "Episode # 3800 \tfinished with reward 0.5855130907901117 \tAverage reward of last 100 episode : 0.5824952591891486\n",
      "Episode # 3900 \tfinished with reward 0.5848849688452032 \tAverage reward of last 100 episode : 0.5908793650657955\n",
      "Episode # 4000 \tfinished with reward 0.5734827174690881 \tAverage reward of last 100 episode : 0.5771481444221702\n",
      "Episode # 4100 \tfinished with reward 0.5930350783939511 \tAverage reward of last 100 episode : 0.5823635395413236\n",
      "Episode # 4200 \tfinished with reward 0.5964260882027054 \tAverage reward of last 100 episode : 0.599145564972098\n",
      "Episode # 4300 \tfinished with reward 0.6243299056855458 \tAverage reward of last 100 episode : 0.6099621236237089\n",
      "Episode # 4400 \tfinished with reward 0.6140378791324077 \tAverage reward of last 100 episode : 0.6183033966499966\n",
      "Episode # 4500 \tfinished with reward 0.6026221440349675 \tAverage reward of last 100 episode : 0.6035937797684031\n",
      "Episode # 4600 \tfinished with reward 0.6018366388179369 \tAverage reward of last 100 episode : 0.597263974051193\n",
      "Episode # 4700 \tfinished with reward 0.6038566467021441 \tAverage reward of last 100 episode : 0.5985482057542331\n",
      "Episode # 4800 \tfinished with reward 0.6198802494452341 \tAverage reward of last 100 episode : 0.6059750048614106\n",
      "Episode # 4900 \tfinished with reward 0.6660817575808272 \tAverage reward of last 100 episode : 0.6264281526816032\n",
      "Episode # 5000 \tfinished with reward 0.6780719713828985 \tAverage reward of last 100 episode : 0.6749460453220388\n",
      "Episode # 5100 \tfinished with reward 0.7268215756814368 \tAverage reward of last 100 episode : 0.7081525768399909\n",
      "Episode # 5200 \tfinished with reward 0.7136359132802431 \tAverage reward of last 100 episode : 0.7208572385087944\n",
      "Episode # 5300 \tfinished with reward 0.7021311321759437 \tAverage reward of last 100 episode : 0.7149109631215644\n",
      "Episode # 5400 \tfinished with reward 0.7203393183338903 \tAverage reward of last 100 episode : 0.7093214112533937\n",
      "Episode # 5500 \tfinished with reward 0.7408739600590712 \tAverage reward of last 100 episode : 0.7060197258380921\n",
      "Episode # 5600 \tfinished with reward 0.741618242180651 \tAverage reward of last 100 episode : 0.74188838574041\n",
      "Episode # 5700 \tfinished with reward 0.7137695101224774 \tAverage reward of last 100 episode : 0.7375631582554505\n",
      "Episode # 5800 \tfinished with reward 0.7652772872343234 \tAverage reward of last 100 episode : 0.7313719150421456\n",
      "Episode # 5900 \tfinished with reward 0.7234635546503508 \tAverage reward of last 100 episode : 0.7378571891733547\n",
      "Episode # 6000 \tfinished with reward 0.7031247720594062 \tAverage reward of last 100 episode : 0.7154528964895802\n",
      "Episode # 6100 \tfinished with reward 0.693662430837124 \tAverage reward of last 100 episode : 0.7169309146388921\n",
      "Episode # 6200 \tfinished with reward 0.6806237105728572 \tAverage reward of last 100 episode : 0.7007715169536932\n",
      "Episode # 6300 \tfinished with reward 0.6403756552613891 \tAverage reward of last 100 episode : 0.6638932492620997\n",
      "Episode # 6400 \tfinished with reward 0.6447235399792198 \tAverage reward of last 100 episode : 0.6383549751463342\n",
      "Episode # 6500 \tfinished with reward 0.6252379431400964 \tAverage reward of last 100 episode : 0.6416446145293022\n",
      "Episode # 6600 \tfinished with reward 0.5866869632830868 \tAverage reward of last 100 episode : 0.6112524108454936\n",
      "Episode # 6700 \tfinished with reward 0.5913012871536475 \tAverage reward of last 100 episode : 0.5954393480604692\n",
      "Episode # 6800 \tfinished with reward 0.5732943767446212 \tAverage reward of last 100 episode : 0.5912756719091151\n",
      "Episode # 6900 \tfinished with reward 0.595671177566874 \tAverage reward of last 100 episode : 0.5850788353776274\n",
      "Episode # 7000 \tfinished with reward 0.5997479920549155 \tAverage reward of last 100 episode : 0.5927786560909291\n",
      "Episode # 7100 \tfinished with reward 0.5950084662491764 \tAverage reward of last 100 episode : 0.6004126869584607\n",
      "Episode # 7200 \tfinished with reward 0.5886354507876053 \tAverage reward of last 100 episode : 0.5906196300156484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode # 7300 \tfinished with reward 0.5917861470226322 \tAverage reward of last 100 episode : 0.5930995583734046\n",
      "Episode # 7400 \tfinished with reward 0.6402919563655224 \tAverage reward of last 100 episode : 0.6108240792144372\n",
      "Episode # 7500 \tfinished with reward 0.6463244508728695 \tAverage reward of last 100 episode : 0.6582492179648005\n",
      "Episode # 7600 \tfinished with reward 0.6221637212963631 \tAverage reward of last 100 episode : 0.6380780851345995\n",
      "Episode # 7700 \tfinished with reward 0.639034430219936 \tAverage reward of last 100 episode : 0.6287793167901452\n",
      "Episode # 7800 \tfinished with reward 0.6739374050864619 \tAverage reward of last 100 episode : 0.6554394622609692\n",
      "Episode # 7900 \tfinished with reward 0.6843694930732882 \tAverage reward of last 100 episode : 0.6906970374953738\n",
      "Episode # 8000 \tfinished with reward 0.6724673605265818 \tAverage reward of last 100 episode : 0.6795789074593384\n",
      "Episode # 8100 \tfinished with reward 0.7232292871420578 \tAverage reward of last 100 episode : 0.7102492236556153\n",
      "Episode # 8200 \tfinished with reward 0.7203464838247079 \tAverage reward of last 100 episode : 0.7166577985422085\n",
      "Episode # 8300 \tfinished with reward 0.7124281664803541 \tAverage reward of last 100 episode : 0.715365447895675\n",
      "Episode # 8400 \tfinished with reward 0.6930087086632758 \tAverage reward of last 100 episode : 0.7015803266242046\n",
      "Episode # 8500 \tfinished with reward 0.7086672565908123 \tAverage reward of last 100 episode : 0.6998588967584431\n",
      "Episode # 8600 \tfinished with reward 0.727028651747529 \tAverage reward of last 100 episode : 0.7258573962892396\n",
      "Episode # 8700 \tfinished with reward 0.7774410477166667 \tAverage reward of last 100 episode : 0.7610004026033527\n",
      "Episode # 8800 \tfinished with reward 0.7228365284512194 \tAverage reward of last 100 episode : 0.7452520589874267\n",
      "Episode # 8900 \tfinished with reward 0.721472635470317 \tAverage reward of last 100 episode : 0.7383017593694554\n",
      "Episode # 9000 \tfinished with reward 0.802769222337064 \tAverage reward of last 100 episode : 0.7741529284506187\n",
      "Episode # 9100 \tfinished with reward 0.8498643387402818 \tAverage reward of last 100 episode : 0.8458772572318606\n",
      "Episode # 9200 \tfinished with reward 0.8475468627367557 \tAverage reward of last 100 episode : 0.8472978355516183\n",
      "Episode # 9300 \tfinished with reward 0.8823379752223338 \tAverage reward of last 100 episode : 0.8702310748449384\n",
      "Episode # 9400 \tfinished with reward 0.9304319306862332 \tAverage reward of last 100 episode : 0.9154172061180965\n",
      "Episode # 9500 \tfinished with reward 0.9841205134464477 \tAverage reward of last 100 episode : 0.9665163319790406\n",
      "Episode # 9600 \tfinished with reward 1.0967653054591204 \tAverage reward of last 100 episode : 1.0273943668245018\n",
      "Episode # 9700 \tfinished with reward 1.2099943665128894 \tAverage reward of last 100 episode : 1.1267061115871528\n",
      "Episode # 9800 \tfinished with reward 1.1276234648124686 \tAverage reward of last 100 episode : 1.1609556134297785\n",
      "Episode # 9900 \tfinished with reward 1.0697797681212777 \tAverage reward of last 100 episode : 1.1120700424039722\n",
      "Episode # 10000 \tfinished with reward 1.153617575308296 \tAverage reward of last 100 episode : 1.1151255730504817\n",
      "Episode # 10100 \tfinished with reward 1.2819253173223053 \tAverage reward of last 100 episode : 1.2320114695207602\n",
      "Episode # 10200 \tfinished with reward 1.226128451139754 \tAverage reward of last 100 episode : 1.2773873390427468\n",
      "Episode # 10300 \tfinished with reward 1.2092799598293467 \tAverage reward of last 100 episode : 1.2479744026066313\n",
      "Episode # 10400 \tfinished with reward 1.3829959385707424 \tAverage reward of last 100 episode : 1.2842482992375148\n",
      "Episode # 10500 \tfinished with reward 1.404693575138126 \tAverage reward of last 100 episode : 1.4039258038287357\n",
      "Episode # 10600 \tfinished with reward 1.4258842094906454 \tAverage reward of last 100 episode : 1.3760722572975694\n",
      "Episode # 10700 \tfinished with reward 1.5942858479135926 \tAverage reward of last 100 episode : 1.5337426294176277\n",
      "Episode # 10800 \tfinished with reward 1.8206008284536663 \tAverage reward of last 100 episode : 1.7128112034930467\n",
      "Episode # 10900 \tfinished with reward 2.4501048679112962 \tAverage reward of last 100 episode : 2.0062096458443173\n",
      "Episode # 11000 \tfinished with reward 2.783332522803286 \tAverage reward of last 100 episode : 2.5439294770616288\n",
      "Episode # 11100 \tfinished with reward 4.552041203722033 \tAverage reward of last 100 episode : 3.2036684788387526\n",
      "Episode # 11200 \tfinished with reward 7.669038316776385 \tAverage reward of last 100 episode : 5.071622517507321\n",
      "Episode # 11300 \tfinished with reward 16.683947732841805 \tAverage reward of last 100 episode : 13.153480323840112\n",
      "Episode # 11400 \tfinished with reward 21.941217277016065 \tAverage reward of last 100 episode : 19.075703164068745\n",
      "Episode # 11500 \tfinished with reward 17.785648322994383 \tAverage reward of last 100 episode : 19.104567003566192\n",
      "Episode # 11600 \tfinished with reward 18.137667572288645 \tAverage reward of last 100 episode : 19.04887008806044\n",
      "Episode # 11700 \tfinished with reward 17.52859494325641 \tAverage reward of last 100 episode : 19.043322403477024\n",
      "Episode # 11800 \tfinished with reward 21.273130037754882 \tAverage reward of last 100 episode : 19.553607945618605\n",
      "Episode # 11900 \tfinished with reward 20.021399116439202 \tAverage reward of last 100 episode : 19.606637632759185\n",
      "Episode # 12000 \tfinished with reward 21.659678994626095 \tAverage reward of last 100 episode : 19.647211734469078\n"
     ]
    }
   ],
   "source": [
    "ag = Agent()\n",
    "ag.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00934278] [[-0.0090393]]\n"
     ]
    }
   ],
   "source": [
    "action, action_matrix, predicted_action = ag.get_action_continuous()\n",
    "print(action, predicted_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.86121804])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ag.observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R: 0.17527005408275634\n"
     ]
    }
   ],
   "source": [
    "observation, reward, done, info = ag.env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00263249])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
