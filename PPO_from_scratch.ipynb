{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = 1000000\n",
    "\n",
    "LOSS_CLIPPING = 0.2 # Only implemented clipping for the surrogate loss, paper said it was best\n",
    "EPOCHS = 10\n",
    "GAMMA = 0.99\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "NUM_ACTIONS = 2\n",
    "NUM_STATE = 1\n",
    "HIDDEN_SIZE = 256\n",
    "ENTROPY_LOSS = 5 * 1e-3 # Does not converge without entropy penalty\n",
    "LR = 1e-4 # Lower lr stabilises training greatly\n",
    "\n",
    "DUMMY_ACTION, DUMMY_VALUE = np.zeros((1, NUM_ACTIONS)), np.zeros((1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.action_space = (1, )\n",
    "        self.observation_space = (1, )\n",
    "    \n",
    "    def reset(self):\n",
    "        self.count = 0\n",
    "        self.state = np.array(np.random.randint(0,2))\n",
    "        self.state = np.array(0)\n",
    "        return np.array([self.state])\n",
    "    \n",
    "    def step(self, action):\n",
    "        if action == self.state:\n",
    "            reward = 1.\n",
    "        else:\n",
    "            reward = -1.\n",
    "            \n",
    "        #self.state = np.array(np.random.randint(0,2))\n",
    "        self.count += 1\n",
    "        done = False\n",
    "        \n",
    "        if self.count >= 10:\n",
    "            done = True\n",
    "        return self.state, reward, done, []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proximal_policy_optimization_loss(advantage, old_prediction):\n",
    "    def loss(y_true, y_pred):\n",
    "        prob = K.sum(y_true * y_pred)\n",
    "        old_prob = K.sum(y_true * old_prediction)\n",
    "        r = prob/(old_prob + 1e-10)\n",
    "\n",
    "        return -K.mean(K.minimum(r * advantage, K.clip(r, min_value=1 - LOSS_CLIPPING, max_value=1 + LOSS_CLIPPING) * advantage)) + ENTROPY_LOSS * (prob * K.log(prob + 1e-10))\n",
    "    return loss\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.critic = self.build_critic()\n",
    "        self.actor = self.build_actor()\n",
    "\n",
    "        self.env = Environment()\n",
    "        self.episode = 0\n",
    "        self.observation = self.env.reset()\n",
    "        self.val = False\n",
    "        self.reward = []\n",
    "        self.reward_over_time = []\n",
    "        self.gradient_steps = 0\n",
    "\n",
    "    def build_actor(self):\n",
    "\n",
    "        state_input = Input(shape=(NUM_STATE,))\n",
    "        advantage = Input(shape=(1,))\n",
    "        old_prediction = Input(shape=(NUM_ACTIONS,))\n",
    "\n",
    "        x = Dense(HIDDEN_SIZE, activation='relu')(state_input)\n",
    "        x = Dropout(0.5)(x)\n",
    "        x = Dense(HIDDEN_SIZE, activation='relu')(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "\n",
    "        out_actions = Dense(NUM_ACTIONS, activation='softmax', name='output')(x)\n",
    "\n",
    "        model = Model(inputs=[state_input, advantage, old_prediction], outputs=[out_actions])\n",
    "        model.compile(optimizer=Adam(lr=LR),\n",
    "                      loss=[proximal_policy_optimization_loss(\n",
    "                          advantage=advantage,\n",
    "                          old_prediction=old_prediction)])\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "    def build_critic(self):\n",
    "\n",
    "        state_input = Input(shape=(NUM_STATE,))\n",
    "        x = Dense(HIDDEN_SIZE, activation='relu')(state_input)\n",
    "        x = Dropout(0.5)(x)\n",
    "        x = Dense(HIDDEN_SIZE, activation='relu')(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "\n",
    "        out_value = Dense(1)(x)\n",
    "\n",
    "        model = Model(inputs=[state_input], outputs=[out_value])\n",
    "        model.compile(optimizer=Adam(lr=LR), loss='mse')\n",
    "\n",
    "        return model\n",
    "\n",
    "    def reset_env(self):\n",
    "        self.episode += 1\n",
    "        if self.episode % 100 == 0:\n",
    "            self.val = True\n",
    "        else:\n",
    "            self.val = False\n",
    "        self.observation = self.env.reset()\n",
    "        self.reward = []\n",
    "\n",
    "    def get_action(self):\n",
    "        p = self.actor.predict([self.observation.reshape(1, NUM_STATE), DUMMY_VALUE, DUMMY_ACTION])\n",
    "        if self.val is False:\n",
    "            action = np.random.choice(NUM_ACTIONS, p=np.nan_to_num(p[0]))\n",
    "        else:\n",
    "            action = np.argmax(np.nan_to_num(p[0]))\n",
    "        action_matrix = np.zeros(NUM_ACTIONS)\n",
    "        action_matrix[action] = 1\n",
    "        return action, action_matrix, p\n",
    "\n",
    "    def transform_reward(self):\n",
    "        for j in range(len(self.reward) - 2, -1, -1):\n",
    "            self.reward[j] += self.reward[j + 1] * GAMMA\n",
    "\n",
    "    def get_batch(self):\n",
    "        batch = [[], [], [], []]\n",
    "\n",
    "        tmp_batch = [[], [], []]\n",
    "        while len(batch[0]) < BATCH_SIZE:\n",
    "            action, action_matrix, predicted_action = self.get_action()\n",
    "            observation, reward, done, info = self.env.step(action)\n",
    "            self.reward.append(reward)\n",
    "\n",
    "            tmp_batch[0].append(self.observation)\n",
    "            tmp_batch[1].append(action_matrix)\n",
    "            tmp_batch[2].append(predicted_action)\n",
    "            self.observation = observation\n",
    "\n",
    "            if done:\n",
    "                self.transform_reward()\n",
    "                for i in range(len(tmp_batch[0])):\n",
    "                    obs, action, pred = tmp_batch[0][i], tmp_batch[1][i], tmp_batch[2][i]\n",
    "                    r = self.reward[i]\n",
    "                    batch[0].append(obs)\n",
    "                    batch[1].append(action)\n",
    "                    batch[2].append(pred)\n",
    "                    batch[3].append(r)\n",
    "                tmp_batch = [[], [], []]\n",
    "                self.reset_env()\n",
    "\n",
    "        obs, action, pred, reward = np.array(batch[0]), np.array(batch[1]), np.array(batch[2]), np.reshape(np.array(batch[3]), (len(batch[3]), 1))\n",
    "        pred = np.reshape(pred, (pred.shape[0], pred.shape[2]))\n",
    "        return obs, action, pred, reward\n",
    "\n",
    "    def run(self):\n",
    "        while self.episode < EPISODES:\n",
    "            print(self.episode)\n",
    "            obs, action, pred, reward = self.get_batch()\n",
    "            old_prediction = pred\n",
    "            pred_values = self.critic.predict(obs)\n",
    "\n",
    "            advantage = reward - pred_values\n",
    "\n",
    "            actor_loss = []\n",
    "            critic_loss = []\n",
    "            for e in range(EPOCHS):\n",
    "                actor_loss.append(self.actor.train_on_batch([obs, advantage, old_prediction], [action]))\n",
    "                critic_loss.append(self.critic.train_on_batch([obs], [reward]))\n",
    "\n",
    "            self.gradient_steps += 1\n",
    "            \n",
    "            for i in range(2):\n",
    "                obs = np.array(i)\n",
    "                p = self.actor.predict([obs.reshape(1, NUM_STATE), DUMMY_VALUE, DUMMY_ACTION])\n",
    "                print(obs, p)\n",
    "                \n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0 [[0.50000465 0.49999532]]\n",
      "1 [[0.4895404 0.5104596]]\n",
      "\n",
      "26\n",
      "0 [[0.5000061  0.49999392]]\n",
      "1 [[0.4895418 0.5104582]]\n",
      "\n",
      "52\n",
      "0 [[0.5000072  0.49999282]]\n",
      "1 [[0.48954293 0.51045704]]\n",
      "\n",
      "78\n",
      "0 [[0.5000047  0.49999532]]\n",
      "1 [[0.4895404 0.5104596]]\n",
      "\n",
      "104\n",
      "0 [[0.50000304 0.49999693]]\n",
      "1 [[0.48953876 0.5104612 ]]\n",
      "\n",
      "130\n",
      "0 [[0.49999988 0.5000001 ]]\n",
      "1 [[0.48953563 0.5104644 ]]\n",
      "\n",
      "156\n",
      "0 [[0.49999422 0.5000058 ]]\n",
      "1 [[0.48952997 0.51047003]]\n",
      "\n",
      "182\n",
      "0 [[0.49999693 0.50000304]]\n",
      "1 [[0.48953268 0.5104673 ]]\n",
      "\n",
      "208\n",
      "0 [[0.4999984 0.5000016]]\n",
      "1 [[0.48953414 0.51046586]]\n",
      "\n",
      "234\n",
      "0 [[0.49999908 0.50000095]]\n",
      "1 [[0.48953483 0.5104652 ]]\n",
      "\n",
      "260\n",
      "0 [[0.4999999  0.50000006]]\n",
      "1 [[0.48953566 0.5104643 ]]\n",
      "\n",
      "286\n",
      "0 [[0.5000024  0.49999765]]\n",
      "1 [[0.48953807 0.5104619 ]]\n",
      "\n",
      "312\n",
      "0 [[0.5000052  0.49999484]]\n",
      "1 [[0.48954087 0.51045907]]\n",
      "\n",
      "338\n",
      "0 [[0.5000069  0.49999312]]\n",
      "1 [[0.48954263 0.5104574 ]]\n",
      "\n",
      "364\n",
      "0 [[0.50000674 0.4999933 ]]\n",
      "1 [[0.48954245 0.5104575 ]]\n",
      "\n",
      "390\n",
      "0 [[0.5000037  0.49999633]]\n",
      "1 [[0.48953938 0.51046056]]\n",
      "\n",
      "416\n",
      "0 [[0.5000064 0.4999936]]\n",
      "1 [[0.48954213 0.5104579 ]]\n",
      "\n",
      "442\n",
      "0 [[0.50001305 0.49998698]]\n",
      "1 [[0.48954877 0.51045126]]\n",
      "\n",
      "468\n",
      "0 [[0.5000171  0.49998292]]\n",
      "1 [[0.48955283 0.51044714]]\n",
      "\n",
      "494\n",
      "0 [[0.5000194  0.49998063]]\n",
      "1 [[0.48955512 0.5104449 ]]\n",
      "\n",
      "520\n",
      "0 [[0.50002015 0.49997988]]\n",
      "1 [[0.48955587 0.51044416]]\n",
      "\n",
      "546\n",
      "0 [[0.5000177  0.49998233]]\n",
      "1 [[0.48955342 0.51044655]]\n",
      "\n",
      "572\n",
      "0 [[0.5000125  0.49998745]]\n",
      "1 [[0.48954824 0.51045173]]\n",
      "\n",
      "598\n",
      "0 [[0.5000054 0.4999946]]\n",
      "1 [[0.48954114 0.5104589 ]]\n",
      "\n",
      "624\n",
      "0 [[0.50000167 0.49999833]]\n",
      "1 [[0.4895374 0.5104626]]\n",
      "\n",
      "650\n",
      "0 [[0.4999992  0.50000083]]\n",
      "1 [[0.4895349 0.5104651]]\n",
      "\n",
      "676\n",
      "0 [[0.49999708 0.5000029 ]]\n",
      "1 [[0.4895328 0.5104672]]\n",
      "\n",
      "702\n",
      "0 [[0.49999923 0.5000008 ]]\n",
      "1 [[0.48953497 0.510465  ]]\n",
      "\n",
      "728\n",
      "0 [[0.5000015  0.49999854]]\n",
      "1 [[0.4895372 0.5104628]]\n",
      "\n",
      "754\n",
      "0 [[0.5000058  0.49999425]]\n",
      "1 [[0.4895415  0.51045847]]\n",
      "\n",
      "780\n",
      "0 [[0.50000787 0.49999216]]\n",
      "1 [[0.4895436  0.51045644]]\n",
      "\n",
      "806\n",
      "0 [[0.500009   0.49999103]]\n",
      "1 [[0.48954472 0.5104553 ]]\n",
      "\n",
      "832\n",
      "0 [[0.5000104 0.4999896]]\n",
      "1 [[0.4895461 0.5104539]]\n",
      "\n",
      "858\n",
      "0 [[0.50001335 0.49998668]]\n",
      "1 [[0.48954904 0.51045096]]\n",
      "\n",
      "884\n",
      "0 [[0.5000136  0.49998644]]\n",
      "1 [[0.4895493 0.5104507]]\n",
      "\n",
      "910\n",
      "0 [[0.50001216 0.49998784]]\n",
      "1 [[0.4895479 0.5104521]]\n",
      "\n",
      "936\n",
      "0 [[0.5000108  0.49998924]]\n",
      "1 [[0.48954648 0.5104535 ]]\n",
      "\n",
      "962\n",
      "0 [[0.5000072  0.49999282]]\n",
      "1 [[0.48954293 0.5104571 ]]\n",
      "\n",
      "988\n",
      "0 [[0.5000018  0.49999824]]\n",
      "1 [[0.4895375 0.5104625]]\n",
      "\n",
      "1014\n",
      "0 [[0.4999973  0.50000274]]\n",
      "1 [[0.48953304 0.510467  ]]\n",
      "\n",
      "1040\n",
      "0 [[0.4999946 0.5000054]]\n",
      "1 [[0.48953032 0.5104697 ]]\n",
      "\n",
      "1066\n",
      "0 [[0.4999896 0.5000104]]\n",
      "1 [[0.48952535 0.5104746 ]]\n",
      "\n",
      "1092\n",
      "0 [[0.49999002 0.50001   ]]\n",
      "1 [[0.48952574 0.51047426]]\n",
      "\n",
      "1118\n",
      "0 [[0.49999496 0.50000507]]\n",
      "1 [[0.4895307  0.51046926]]\n",
      "\n",
      "1144\n",
      "0 [[0.49999815 0.50000185]]\n",
      "1 [[0.4895339 0.5104661]]\n",
      "\n",
      "1170\n",
      "0 [[0.49999776 0.5000022 ]]\n",
      "1 [[0.48953354 0.51046646]]\n",
      "\n",
      "1196\n",
      "0 [[0.49999937 0.50000066]]\n",
      "1 [[0.4895351 0.5104649]]\n",
      "\n",
      "1222\n",
      "0 [[0.50000197 0.49999806]]\n",
      "1 [[0.48953766 0.51046234]]\n",
      "\n",
      "1248\n",
      "0 [[0.50000316 0.49999687]]\n",
      "1 [[0.48953888 0.51046115]]\n",
      "\n",
      "1274\n",
      "0 [[0.5000061  0.49999395]]\n",
      "1 [[0.4895418 0.5104582]]\n",
      "\n",
      "1300\n",
      "0 [[0.50000805 0.49999198]]\n",
      "1 [[0.48954377 0.5104562 ]]\n",
      "\n",
      "1326\n",
      "0 [[0.5000107 0.4999893]]\n",
      "1 [[0.48954642 0.5104536 ]]\n",
      "\n",
      "1352\n",
      "0 [[0.50001353 0.49998644]]\n",
      "1 [[0.48954928 0.5104507 ]]\n",
      "\n",
      "1378\n",
      "0 [[0.50001514 0.49998486]]\n",
      "1 [[0.4895509 0.5104492]]\n",
      "\n",
      "1404\n",
      "0 [[0.5000167  0.49998334]]\n",
      "1 [[0.4895524 0.5104476]]\n",
      "\n",
      "1430\n",
      "0 [[0.50001854 0.49998146]]\n",
      "1 [[0.48955423 0.5104457 ]]\n",
      "\n",
      "1456\n",
      "0 [[0.5000211  0.49997887]]\n",
      "1 [[0.48955685 0.51044315]]\n",
      "\n",
      "1482\n",
      "0 [[0.5000237  0.49997628]]\n",
      "1 [[0.48955944 0.5104405 ]]\n",
      "\n",
      "1508\n",
      "0 [[0.5000272 0.4999728]]\n",
      "1 [[0.48956293 0.5104371 ]]\n",
      "\n",
      "1534\n",
      "0 [[0.50002515 0.49997485]]\n",
      "1 [[0.48956087 0.5104391 ]]\n",
      "\n",
      "1560\n",
      "0 [[0.50001925 0.49998078]]\n",
      "1 [[0.4895549  0.51044506]]\n",
      "\n",
      "1586\n",
      "0 [[0.5000127  0.49998727]]\n",
      "1 [[0.48954841 0.51045156]]\n",
      "\n",
      "1612\n",
      "0 [[0.50000936 0.4999906 ]]\n",
      "1 [[0.4895451  0.51045495]]\n",
      "\n",
      "1638\n",
      "0 [[0.5000069  0.49999312]]\n",
      "1 [[0.48954263 0.5104574 ]]\n",
      "\n",
      "1664\n",
      "0 [[0.5000046  0.49999544]]\n",
      "1 [[0.4895403 0.5104597]]\n",
      "\n",
      "1690\n",
      "0 [[0.5000029 0.4999971]]\n",
      "1 [[0.48953864 0.51046133]]\n",
      "\n",
      "1716\n",
      "0 [[0.49999925 0.5000007 ]]\n",
      "1 [[0.48953497 0.51046497]]\n",
      "\n",
      "1742\n",
      "0 [[0.49999842 0.5000016 ]]\n",
      "1 [[0.48953417 0.5104658 ]]\n",
      "\n",
      "1768\n",
      "0 [[0.49999735 0.5000026 ]]\n",
      "1 [[0.48953313 0.5104669 ]]\n",
      "\n",
      "1794\n",
      "0 [[0.49999616 0.5000038 ]]\n",
      "1 [[0.4895319 0.5104681]]\n",
      "\n",
      "1820\n",
      "0 [[0.49999198 0.50000805]]\n",
      "1 [[0.4895277  0.51047224]]\n",
      "\n",
      "1846\n",
      "0 [[0.49999025 0.5000098 ]]\n",
      "1 [[0.48952597 0.510474  ]]\n",
      "\n",
      "1872\n"
     ]
    }
   ],
   "source": [
    "EPISODES = 10000\n",
    "\n",
    "LOSS_CLIPPING = 0.2 # Only implemented clipping for the surrogate loss, paper said it was best\n",
    "EPOCHS = 10\n",
    "GAMMA = 0.99\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "NUM_ACTIONS = 2\n",
    "NUM_STATE = 1\n",
    "HIDDEN_SIZE = 256\n",
    "ENTROPY_LOSS = 5 * 1e-3 # Does not converge without entropy penalty\n",
    "LR = 1e-6 # Lower lr stabilises training greatly\n",
    "\n",
    "DUMMY_ACTION, DUMMY_VALUE = np.zeros((1, NUM_ACTIONS)), np.zeros((1, 1))\n",
    "\n",
    "agent = Agent()\n",
    "agent.run()\n",
    "env = Environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
